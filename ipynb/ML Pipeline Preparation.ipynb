{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords'])\n",
    "    \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(**kwargs):\n",
    "    \"\"\"\n",
    "    Builds machine learning pipeline, sets parameters for components if given.\n",
    "    :param text: free text\n",
    "    :return tokenized words\n",
    "    \"\"\"\n",
    "    \n",
    "    tf_args = {}\n",
    "    clf_args = {}\n",
    "    vect_args = {}\n",
    "    \n",
    "    classifier = kwargs.get('classifier', DecisionTreeClassifier())\n",
    "\n",
    "    for key, value in kwargs.items() :\n",
    "        if key.startswith('tf'):\n",
    "            tf_args.update({key.split('__')[-1] :  value})\n",
    "        elif key.startswith('clf'):\n",
    "            clf_args.update({key.split('__')[-1] :  value})\n",
    "        elif key.startswith('vect'):\n",
    "            vect_args.update({key.split('__')[-1] :  value})            \n",
    "            \n",
    "    pipeline = Pipeline([('vect', CountVectorizer(tokenizer=tokenize, **vect_args)),\n",
    "                         ('tfidf', TfidfTransformer(**tf_args)),\n",
    "                         ('clf', MultiOutputClassifier(classifier.set_params(**clf_args)))\n",
    "                        ])\n",
    "\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    Returns model dumped in pickle file\n",
    "    :param filename: model pickle file\n",
    "    :return model in pickle\n",
    "    \"\"\"\n",
    "    return pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "def load_data(dbname, tablename='messages'):\n",
    "    \"\"\"\n",
    "    Loads data saved in db.table\n",
    "    :param dbname: sql db name\n",
    "    :param tablename: sql table name\n",
    "    :return input and label data as tuple\n",
    "    \"\"\"\n",
    "    engine = create_engine('sqlite:///{}'.format(dbname))\n",
    "    df = pd.read_sql_table(tablename, con=engine)\n",
    "    X = df['message']\n",
    "    Y = df.drop(['id','original','message', 'genre'], axis=1)\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Removes special characters, stopwords.First lemmatizes and then stems the tokens\n",
    "    :param text: free text\n",
    "    :return tokenized words\n",
    "    \"\"\"\n",
    "    \n",
    "    text = re.sub('[^(a-zA-Z0-9)]',' ',text.lower())\n",
    "    words = word_tokenize(text)\n",
    "    clean= [word for word in words if word not in stopwords.words(\"english\") ]\n",
    "    lemmed = [WordNetLemmatizer().lemmatize(w) for w in clean]\n",
    "    stemmed = [PorterStemmer().stem(w) for w in lemmed]\n",
    "    \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_estimator(pipeline, param_grid, X_train, Y_train):\n",
    "    \"\"\"\n",
    "    Implements grid search for given param_grid on pipeline\n",
    "    :param pipeline: sklearn pipeline\n",
    "    :param param_grid: Dictionary with parameters names and values\n",
    "    :param X_train: Training Input Data\n",
    "    :param Y_train: Training Label Data\n",
    "    :return best estimator\n",
    "    \"\"\"\n",
    "\n",
    "    cv = GridSearchCV(pipeline, param_grid=param_grid, n_jobs=-1)\n",
    "    cv.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"Grid Search results for pipeline:\\n best parameters : {}\".format(cv.best_params_))\n",
    "    return cv.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_scores(pipeline, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    Runs the model on test dataset\n",
    "    Calculates f1 score, precision and recall for each category\n",
    "    :param pipeline: trained ML pipeline\n",
    "    :param X_test: Test Input Data\n",
    "    :param Y_test: Test Label Data\n",
    "    :returns pandas dataframe with metrics for each class of each category\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Y_preds = pipeline.predict(X_test)\n",
    "    Y_preds = pd.DataFrame(Y_preds, columns=Y_test.columns)\n",
    "    \n",
    "    report = []\n",
    "    for col in Y_test.columns.tolist():\n",
    "        report.append({\n",
    "            'category': col,\n",
    "            'precision': precision_score(Y_test[col], Y_preds[col], average='micro'),\n",
    "            'recall': recall_score(Y_test[col], Y_preds[col], average='micro'),\n",
    "            'f1_score': f1_score(Y_test[col], Y_preds[col], average='micro')})\n",
    "\n",
    "    df_report = pd.DataFrame(report) \n",
    "    return df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pickle(model, filename):\n",
    "#     filename = 'final_model.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  main \n",
    "X, Y = load_data('data/disaster.db', 'messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search takes significant time to find best estimator\n",
    "# parameters = {\n",
    "#         'tfidf__norm': ['l1', 'l2'],\n",
    "#         'clf__estimator__max_depth': [12, None]  \n",
    "# }\n",
    "\n",
    "# pipeline_0 = build_pipeline()\n",
    "\n",
    "# pipeline = get_best_estimator(pipeline=pipeline_0, param_grid=parameters, X_train=X_train, Y_train=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'clf__estimator__max_depth': 12, 'tfidf__norm': 'l2'}\n",
    "pipeline = build_pipeline(**best_params)\n",
    "\n",
    "pipeline.fit(X_train, Y_train)\n",
    "df_report_dtree = report_scores(pipeline, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves report as csv\n",
    "# df_report_dtree.to_csv('decision_tree_improved_report.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_2 = clone(pipeline)\n",
    "pipeline_2.steps.pop(2)\n",
    "pipeline_2.steps.append(['clf_new',MultiOutputClassifier(RandomForestClassifier(random_state=13, n_estimators=50))])\n",
    "\n",
    "pipeline_2.fit(X_train, Y_train)\n",
    "df_report_rf = report_scores(pipeline_2, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read previously generated report for improved pipeline with decision tree classifier\n",
    "df_report_dtree = pd.read_csv('decision_tree_improved_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = df_report_dtree.merge(df_report_rf, on='category')\n",
    "merged.drop('Unnamed: 0' , axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['diff_f1'] = merged.f1_score_y-merged.f1_score_x\n",
    "merged['diff_precision'] = merged.precision_y-merged.precision_x\n",
    "merged['diff_recall'] = merged.recall_y-merged.recall_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category          relatedrequestofferaid_relatedmedical_helpmedi...\n",
       "f1_score_x                                                  34.0337\n",
       "precision_x                                                 34.0337\n",
       "recall_x                                                    34.0337\n",
       "f1_score_y                                                   34.126\n",
       "precision_y                                                  34.126\n",
       "recall_y                                                     34.126\n",
       "diff_f1                                                   0.0923077\n",
       "diff_precision                                            0.0923077\n",
       "diff_recall                                               0.0923077\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pickle(pipeline_2, 'model.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
